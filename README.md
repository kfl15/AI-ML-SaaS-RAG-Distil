# AI-ML-SaaS-RAG-Distil

A fully local, reproducible **RAG-based dataset distillation pipeline** designed to generate
high-quality instruction datasets for **LoRA / QLoRA fine-tuning**.

This project uses an existing RAG system as a **teacher** to produce grounded questionâ€“answer
pairs from documents, with strict filtering to suppress hallucinations.

---

## âœ… Key Features

- Fully **local execution** (WSL + Windows Ollama)
- No external APIs
- Deterministic, step-by-step pipeline
- Model-agnostic (Gemma, Mistral tested)
- Produces **instruction-ready JSONL datasets**
- Git-clean, reproducible workflow

---

## ğŸ“ Project Structure

.
â”œâ”€â”€ raw_docs/ # Input PDFs / TXTs (not committed)
â”œâ”€â”€ data_2/ # Chroma vector DB (local)
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ 1_generate_questions.py
â”‚ â”œâ”€â”€ 2_run_rag_answers.py
â”‚ â”œâ”€â”€ 3_filter_samples.py
â”‚ â””â”€â”€ 4_format_dataset.py
â”œâ”€â”€ filtered/
â”‚ â”œâ”€â”€ final_dataset.jsonl
â”‚ â””â”€â”€ final_dataset_instruct.jsonl
â”œâ”€â”€ populate_database.py
â”œâ”€â”€ query_data.py
â”œâ”€â”€ get_embedding_function.py
â”œâ”€â”€ run_pipeline.sh # One-command runner
â””â”€â”€ README.md


## âš™ï¸ Requirements

- Windows 10/11
- WSL (Ubuntu)
- Python 3.11
- Ollama (Windows)
- Models:
  - `nomic-embed-text`
  - `gemma:2b` (or `mistral`)

---

## ğŸš€ One-Command Run

After cloning the repo,ensure placing documents in `raw_docs/`as pdfs, txt. 


## ğŸ“¦ Python Dependencies

All required Python packages are listed in `requirements.txt`.

## â–¶ï¸ How to Run

1. Create and activate your own Python virtual environment
2. Ensure Ollama is running
3. Place documents in `raw_docs/`
4. Run:

```bash
./run_pipeline.sh